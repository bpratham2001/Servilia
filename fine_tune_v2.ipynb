{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOslbNBP/SiJ13zC/EVPyzI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bpratham2001/Servilia/blob/main/fine_tune_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "#nn = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\""
      ],
      "metadata": {
        "id": "Ivkduj04ixUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_3AwLd8Wio2A"
      },
      "outputs": [],
      "source": [
        "# @title Load data & imports\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "import pandas as pd\n",
        "from random import randint as r\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.distributed as dist\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "from torch.distributed.fsdp import ShardingStrategy\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "#from google.colab import output\n",
        "\n",
        "file =('wine-tastings-guide.txt')\n",
        "df1 = pd.read_csv(file, sep = ' - ')\n",
        "file =('FR-IT.txt')\n",
        "df2 = pd.read_csv(file, sep = ' - ')\n",
        "file =('grapes.txt')\n",
        "df3 = pd.read_csv(file, sep = ' - ')\n",
        "file =('wine-enthusiast.txt') # Questions already framed\n",
        "df4 = pd.read_csv(file, sep = ' - ')\n",
        "file =('essence-of-wine.txt') # Questions already framed\n",
        "df5 = pd.read_csv(file, sep = ' - ')\n",
        "#output.clear\n",
        "print(\"total data points available: \" + str(df1.shape[0]+df2.shape[0]+df3.shape[0]+df4.shape[0]+df5.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sample data\n",
        "\n",
        "sample = pd.read_json(\"hf://datasets/yahma/alpaca-cleaned/alpaca_data_cleaned.json\")\n",
        "\n",
        "# prompts\n",
        "tastes = [\"Tell me more about wines that are\", \"Describe to me wines that feel\",\n",
        "          \"I would like to know more about wines considered\", \"I want to know more about wines that are\",\n",
        "          \"On wines, tell me the meaning of\", \"Explain, in wine terminology,\", \"Elaborate on wines that feel\", \"Enlighten me on wines that feel\",\n",
        "          \"Educate me on wines that are\", \"I don't understand wines that are\"]\n",
        "tastes_q = [\"What does it mean when a wine is\", \"How can a wine be\", \"How are wines considered\", \"How are wines\"]\n",
        "tastes_q2 = [\"Why does\", \"How can\", \"Why would\", \"How does\", \"What does it mean if\"]\n",
        "tastes_q3 = [\"feel \", \"taste \", \"seem \"]\n",
        "other = [\"Tell me more about\", \"Describe\", \"I would like to know more about\", \"I want to know more about\",\n",
        "         \"Tell me about\", \"Explain\", \"Elaborate on\", \"Enlighten me on\", \"Educate me on\", \"Recommend me \", \"Give me a recommendation regarding \"]\n",
        "what_singular = [\"What is\", \"What's\"]\n",
        "\n",
        "#unused\n",
        "what_plural = [\"What are\"]\n",
        "how_singular = [\"How does\", \"How is\", \"How can\", \"How come\"]\n",
        "how_plural = [\"How do\", \"How are\", \"How can\", \"How come\"]\n",
        "where_singular = [\"Where is\", \"What is the location of\", \"Where can I find\", \"How do I come across\"]\n",
        "where_plural = [\"Where are\", \"What are the locations of\"]\n",
        "why_plural = [\"Why do\", \"Why are\"]\n",
        "\n",
        "# function to construct question/answers from blogs/articles\n",
        "def construct_prompt(row, rng, q_type=0):\n",
        "  if q_type == 0:#tastes\n",
        "    if rng == 0:\n",
        "      instruction = str(tastes[r(0, len(tastes)-1)] + \" \" +row['word']+\".\")\n",
        "    elif rng == 1:\n",
        "      instruction = str(tastes_q[r(0, len(tastes_q)-1)] + \" \" +row['word']+\"?\")\n",
        "    else:\n",
        "      instruction = str(tastes_q2[r(0, len(tastes_q2)-2)] + \" a wine \"+ tastes_q3[r(0,len(tastes_q3)-1)] +row['word']+\"?\")\n",
        "  elif q_type == 1:#regions\n",
        "    if rng == 0:\n",
        "      instruction = str(other[r(0, len(other)-1)] + \" \" +row['word']+\".\")\n",
        "    elif rng == 1:\n",
        "      instruction = str(\"How is \" +row['word']+\"?\")\n",
        "    else:\n",
        "      instruction = str(what_singular[r(0, len(what_singular)-1)] + \" \" +row['word']+\" like?\")\n",
        "  else:#grapes\n",
        "    if rng == 0:\n",
        "      instruction = str(other[r(0, len(other)-1)] + \" \" +row['word']+\" grapes.\")\n",
        "    elif rng == 1:\n",
        "      instruction = str(what_singular[r(0, len(what_singular)-1)] + \" \" +row['word']+\"?\")\n",
        "    else:\n",
        "      instruction = str(\"How are the wines that are made from \" +row['word']+\" grapes?\")\n",
        "  output = row['description']\n",
        "  return [instruction, output]\n",
        "\n",
        "#sample.head()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iGl0ZO4gjQJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Formatting data into alpaca format\n",
        "df0 = pd.DataFrame(columns=['instruction', 'input', 'output'])\n",
        "\n",
        "#construct_prompt(row, rng, q_type=0)\n",
        "lst = [df1, df2, df3]\n",
        "for i in range(len(lst)):\n",
        "  for index,row in lst[i].iterrows():\n",
        "    a = construct_prompt(row, r(0,2), i)\n",
        "    df0.loc[len(df0)] = [a[0],'',a[1]]\n",
        "for j in [df4, df5]:\n",
        "  for index,row in j.iterrows():\n",
        "    df0.loc[len(df0)] = [row['question'],'',row['answer']]\n",
        "  if df0.shape[0] < (df1.shape[0]+df2.shape[0]+df3.shape[0]+df4.shape[0]+df5.shape[0]):\n",
        "    df0 = df0.sample(frac=1)\n",
        "df0 = df0.reset_index().drop('index', axis=1)\n",
        "#df0.iloc[50:100]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "f4C2m_zzkLER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data prep\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(nn)\n",
        "tokenizer = AutoTokenizer.from_pretrained(nn)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "dataset = Dataset.from_pandas(df0)\n",
        "\n",
        "# Define a function to apply the chat template\n",
        "def apply_chat_template(example):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": example['instruction']},\n",
        "        {\"role\": \"assistant\", \"content\": example['output']}\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    return {\"prompt\": prompt}\n",
        "\n",
        "# Apply the chat template function to the dataset\n",
        "new_dataset = dataset.map(apply_chat_template)\n",
        "new_dataset = new_dataset.train_test_split(0.05)\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(example):\n",
        "    tokens = tokenizer(example['prompt'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    # Set padding token labels to -100 to ignore them in loss calculation\n",
        "    tokens['labels'] = [\n",
        "        -100 if token == tokenizer.pad_token_id else token for token in tokens['input_ids']\n",
        "    ]\n",
        "    return tokens\n",
        "\n",
        "# Apply tokenize_function to each row\n",
        "tokenized_train = train_dataset.map(tokenize_function)\n",
        "tokenized_train = tokenized_train.remove_columns(['instruction', 'output', 'prompt'])\n",
        "tokenized_test = test_dataset.map(tokenize_function)\n",
        "tokenized_test = tokenized_test.remove_columns(['instruction', 'output', 'prompt'])\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_train, batch_size=2, shuffle=True)\n",
        "test_dataloader = DataLoader(tokenized_test, batch_size=2)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9q-vL3Ekkc6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title FSDP\n",
        "\n",
        "def setup_distributed():\n",
        "    dist.init_process_group(backend=\"nccl\")\n",
        "def cleanup_distributed():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "setup_distributed()\n",
        "model = FSDP(model)\n",
        "model = FSDP(\n",
        "    model,\n",
        "    sharding_strategy=ShardingStrategy.FULL_SHARD,\n",
        "    mixed_precision=True,\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0IB59Enbp4sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "cleanup_distributed()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SOIRzmB-qTA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Testing\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "\n",
        "    accuracy = correct_predictions / len(dataloader.dataset)\n",
        "    return total_loss / len(dataloader), accuracy\n",
        "\n",
        "test_loss, test_accuracy = evaluate(model, test_dataloader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zZSc3wgJrWiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Saving\n",
        "\n"
      ],
      "metadata": {
        "id": "6TRl9xPNsDeY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}